{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib \n",
    "from scipy import optimize\n",
    "from scipy import stats\n",
    "plt.style.use(\"ggplot\")\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "#plt.rcParams['font.family'] = 'IPAexGothic'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # warningが出ないように設定\n",
    "pd.set_option(\"display.max_rows\", None) # pandasの表示上限をなくす\n",
    "pd.set_option(\"display.max_columns\", None) # pandasの表示上限をなくす\n",
    "import pickle\n",
    "import gspread\n",
    "import json\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import collections\n",
    "import gensim\n",
    "import random\n",
    "\n",
    "model = gensim.models.Word2Vec.load(\"../latest-ja-word2vec-gensim-model/word2vec.gensim.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2464"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open(\"../fact_relation_vec_list.binaryfile\",'rb')\n",
    "#f = open(\"../kansei_relation_vec_list.binaryfile\",'rb')\n",
    "mean_vec_list = pickle.load(f)\n",
    "\n",
    "mean_vec_list = random.sample(mean_vec_list, int(len(mean_vec_list)/30))\n",
    "len(mean_vec_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Google Spread Sheetsにアクセス\n",
    "def connect_gspread(jsonf,key):\n",
    "    scope = ['https://spreadsheets.google.com/feeds','https://www.googleapis.com/auth/drive']\n",
    "    credentials = ServiceAccountCredentials.from_json_keyfile_name(jsonf, scope)\n",
    "    gc = gspread.authorize(credentials)\n",
    "    SPREADSHEET_KEY = key\n",
    "    worksheet = gc.open_by_key(SPREADSHEET_KEY).sheet1\n",
    "    return worksheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_vec_list_generator(imput_word_list):\n",
    "    result_vec_list  = []\n",
    "\n",
    "    for w in imput_word_list:\n",
    "\n",
    "        try:\n",
    "            w_to_v = model.wv[w]\n",
    "\n",
    "            for mean_vec in mean_vec_list:\n",
    "                result_vec_plus = w_to_v - mean_vec \n",
    "                result_vec_minus =   w_to_v + mean_vec \n",
    "    \n",
    "                result_vec_list.append(result_vec_plus)\n",
    "                result_vec_list.append(result_vec_minus)\n",
    "        except:\n",
    "            print(\"Error\", w)\n",
    "\n",
    "    return result_vec_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 関係ベクトルと類似度が高い単語をn個上位表示\n",
    "def output_word_list_generator(result_vec_list):\n",
    "    output_word_list = []\n",
    "    max_n = 5\n",
    "\n",
    "    for vec in result_vec_list:\n",
    "        most_similar = np.array(list(model.wv.most_similar([vec], [], max_n)))\n",
    "        #output_word_list.extend(most_similar[:,0].tolist())\n",
    "        \n",
    "        a = most_similar[:,1]\n",
    "        for cos in a:\n",
    "            if float(cos) >= 0.85: #コサイン類似度が0.8以下を除去\n",
    "                output_word_list.extend(most_similar[:,0].tolist())\n",
    "                \n",
    "    return output_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ggg(list_taple):\n",
    "    l = []\n",
    "    for taple in list_taple:\n",
    "        a = taple[0]\n",
    "        l.append(a)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ここでjsonfile名と2-2で用意したkeyを入力\n",
    "jsonf = \"quickstart-1595337771922-22c2ceb47d3c.json\"\n",
    "spread_sheet_key = \"1WKpQ4sxL_IP9Fm6551xDRfhxhDLzmgGAG3FCzDFmamI\"\n",
    "worksheet = connect_gspread(jsonf,spread_sheet_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# スプレッドシートのデータを取得するコード\n",
    "# {\"answer\":[imput_word_list], ...}\n",
    "\n",
    "data_dic = {}\n",
    "\n",
    "for i in range(2, 32): #すプレッソシートの行数\n",
    "    answers_list = worksheet.row_values(i)\n",
    "    imput_word_list = answers_list[2:]\n",
    "    answer_word = answers_list[1]\n",
    "    \n",
    "    data_dic[answer_word] = imput_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LIXIL': ['トステム', 'INAX', '新日軽', 'YKK'],\n",
       " 'ハラール': ['イスラム教', 'イスラム', 'アラビア', 'アジア', 'インドネシア'],\n",
       " 'ダイドードリンコ': ['キリンビバレッジ', 'ダイドーブレンド', 'サントリー', '日本たばこ産業', 'JT', 'コカ・コーラ'],\n",
       " '味の素': ['グルタミン酸', '鈴木三郎助'],\n",
       " '六本木': ['スカイツリー', '浅草'],\n",
       " '花王': ['トクホ', 'ヘルシア', 'サッポロビール', 'サッポロ'],\n",
       " '北九州': ['福岡', '八幡', '住友金属工業'],\n",
       " 'EMS': ['シャープ', '台湾', 'イーエムエス', 'Electronics', '日本'],\n",
       " 'シーメンス': ['三菱重工業', 'ドイツ', '三菱', '日立', 'シーメンス', '英国', 'プライメタルズテクノロジーズ'],\n",
       " 'Fuel': ['FCV', '東京'],\n",
       " '本田技研工業': ['日本経済新聞社', '本田技研工業', 'インド', 'ヤマハ発動機'],\n",
       " 'ジヤトコ': ['トヨタ自動車', 'デンソー', 'アイシン精機', '東海理化', '豊田自動織機', '日産自動車'],\n",
       " '出光興産': ['百田尚樹', '出光興産', '出光', '佐三', '明治', '北九州'],\n",
       " 'ローソン': ['三菱商事', '伊藤忠商事', 'ファミリーマート'],\n",
       " '丸紅': ['中国', 'EV', '伊藤忠商事'],\n",
       " '三越': ['伊勢丹', '三越', '大丸', '松坂屋', '高島屋', 'ドコモ'],\n",
       " 'イズミヤ': ['関西', 'セブン', '岡山', '天満屋ストア', '大阪'],\n",
       " 'デニーズ': ['セブン', 'ファミール'],\n",
       " 'TSUTAYA': ['DVD', '東京', '世田谷', 'GINZASIX', '東京', '銀座'],\n",
       " 'セコム': ['ALSOK', '日本', '東京'],\n",
       " '第一興商': ['日本', 'クラウン', 'BGM', 'ジャスダック'],\n",
       " 'ホテルオークラ': ['東京', 'ジ・オークラ・トーキョー', '帝国ホテル', 'パークハイアット東京', '椿山荘'],\n",
       " 'エイチ・アイ・エス': ['長崎', '佐世保', 'ハウステンボス'],\n",
       " 'PISA': ['OECD', 'Programme', '日本'],\n",
       " 'スポーツニッポン': ['毎日新聞', '日本経済新聞社', '読売新聞', 'サンケイ', '産経新聞'],\n",
       " '文藝春秋': ['直木賞', '三十五', '文藝春秋', '芥川賞'],\n",
       " 'グーグル': ['AI', 'オセロ'],\n",
       " 'ソフトバンク': ['ヤフー', '孫正義', 'ジェイフォン', 'ボーダフォン'],\n",
       " '東京': ['埼玉', '神奈川', '浅草', '六本木']}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIXIL\n",
      "['トステム', 'INAX', '新日軽', 'YKK']\n",
      "['LIXIL', 'ストアー', 'JA共済', '積水ハウス', 'グルメシティ', '総合スーパー', '食品スーパー', '宇部興産', 'Witness', 'イトーヨーカ堂', 'エヌ・ティ・ティ', '（株）', '大丸松坂屋百貨店', 'オークワ', '谷保', '森川町', '旭山', '三井鉱山', '三菱マテリアル', '三菱鉱業', '八郷', '垂井町', '浜頓別町', 'デパ地下', '東レ', '筑波銀行']\n",
      "成功**************************\n",
      "　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　\n",
      "ハラール\n",
      "['イスラム教', 'イスラム', 'アラビア', 'アジア', 'インドネシア']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-410bb5a85643>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mresult_vec_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult_vec_list_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimput_word_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0moutput_word_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_word_list_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_vec_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_word_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-68-897d7ec3b00d>\u001b[0m in \u001b[0;36moutput_word_list_generator\u001b[0;34m(result_vec_list)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mvec\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult_vec_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mmost_similar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;31m#output_word_list.extend(most_similar[:,0].tolist())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0mlimited\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors_norm\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrestrict_vocab\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors_norm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m         \u001b[0mdists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlimited\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "for answer_word, imput_word_list in data_dic.items():\n",
    "    print(answer_word)\n",
    "    print(imput_word_list)\n",
    "    \n",
    "    result_vec_list = result_vec_list_generator(imput_word_list)\n",
    "    output_word_list = output_word_list_generator(result_vec_list)\n",
    "    \n",
    "    c = collections.Counter(output_word_list)\n",
    "    dic = dict(c)\n",
    "    dic2 = sorted(dic.items(), key=lambda x:x[1], reverse=True)\n",
    "    l = ggg(dic2[:30])\n",
    "    \n",
    "    for imput_word in imput_word_list:\n",
    "        l = [output_word for output_word in l if output_word != imput_word]\n",
    "    \n",
    "    print(l)\n",
    "    \n",
    "    if answer_word in l: \n",
    "        print(\"成功**************************\")\n",
    "        \n",
    "        count += 1\n",
    "    \n",
    "    print(\"　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　\")\n",
    "    \n",
    "print(\"正答率\", count/len(data_dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for i in range(2, 33): #すプレッソシートの行数\n",
    "    answers_list = worksheet.row_values(i)\n",
    "    imput_word_list = answers_list[2:]\n",
    "    answer_word = answers_list[1]\n",
    "    print(\"answer\", answer_word)\n",
    "    print(\"imput\", imput_word_list)\n",
    "    \n",
    "    result_vec_list = result_vec_list_generator(imput_word_list)\n",
    "    output_word_list = output_word_list_generator(result_vec_list)\n",
    "    \n",
    "    for imput_word in imput_word_list:\n",
    "        output_word_list = [output_word for output_word in output_word_list if output_word != imput_word]\n",
    "    \n",
    "    c = collections.Counter(output_word_list)\n",
    "    dic = dict(c)\n",
    "    dic2 = sorted(dic.items(), key=lambda x:x[1], reverse=True)\n",
    "    l = ggg(dic2[:30])\n",
    "    print(l)\n",
    "    \n",
    "    if answer_word in l: \n",
    "        print(\"成功**************************\")\n",
    "    \n",
    "    print(\"　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 複数の単語で類似度の高いものを上から１０個表示する\n",
    "most_similar = np.array(list(model.wv.most_similar(imput_word_list, [], 10)))[:,0]\n",
    "most_similar.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
